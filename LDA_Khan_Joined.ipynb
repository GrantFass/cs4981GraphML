{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "\n",
    "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "# CUDA = format_cuda_version(CUDA_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# import uuid  # https://docs.python.org/3/library/uuid.html\n",
    "# import structlog  # for event logging\n",
    "# # from dotenv import load_dotenv # enviornment vars if we want\n",
    "\n",
    "# from pygtail import Pygtail\n",
    "# import boto3\n",
    "# from minio import Minio\n",
    "# from dotenv import load_dotenv\n",
    "from datetime import timedelta, datetime\n",
    "from time import sleep\n",
    "from sys import argv\n",
    "# import threading\n",
    "# from smart_open import smart_open\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# setup to import the preprocessor\n",
    "import sys\n",
    "# from botocore.errorfactory import ClientError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "import torch_geometric\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Preprocessing import Preprocessor\n",
    "preprocessor = Preprocessor(0)\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_exists = os.path.exists('khan_joined.csv')\n",
    "khan = pd.DataFrame([])\n",
    "if joined_exists:\n",
    "    khan = pd.read_csv('khan_joined.csv')\n",
    "khan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    computing = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Computing.csv\")\n",
    "    computing = computing.dropna()\n",
    "    computing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    economics = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Economics.csv\")\n",
    "    economics = economics.dropna()\n",
    "    economics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    humanities = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Humanities.csv\")\n",
    "    humanities = humanities.dropna()\n",
    "    humanities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    math = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Math.csv\")\n",
    "    math = math.dropna()\n",
    "    math.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    science = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Science.csv\")\n",
    "    science = science.dropna()\n",
    "    science.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_main = pd.read_csv(\"Datasets\\\\TEDTalksDataset\\\\ted_main.csv\")\n",
    "transcripts = pd.read_csv(\"Datasets\\\\TEDTalksDataset\\\\transcripts.csv\")\n",
    "validation = ted_main.join(transcripts, lsuffix='url', rsuffix='url', sort=True)\n",
    "validation = validation.dropna()\n",
    "validation.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    computing['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    economics['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    humanities['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    math['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    science['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    khan_dfs = [computing, economics, humanities, math, science]\n",
    "    khan = pd.concat(khan_dfs, axis=0)\n",
    "    khan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    # remap the courses to more broad categories: https://stackoverflow.com/a/16476974\n",
    "    labels = ['physics', 'chemistry', 'biology', 'algebra', 'geometry', 'statistics', 'calculus', 'history', 'macroeconomics', 'microeconomics']\n",
    "    for lbl in labels:\n",
    "        for index, row in khan.iterrows():\n",
    "            if lbl in row['course'].lower():\n",
    "                row['course'] = lbl\n",
    "    khan['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    khan.tail(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Identification\n",
    "It is necessary to now identify components of the dataset that can be used for the graphical machine learning. This means identifying Nodes, Edges, Node Features, and Labels. It also includes optionally including edge weights and edge features. For the sake of simplicity I think that I will be forgoing the edge weights and edge features.\n",
    "\n",
    "I am attempting to basically do topic modeling, but without the keywords and topics that would be customary of Latent Dirchlet Allocation. Instead the goal is to train a Graph ML model using the Khan academy data.\n",
    "\n",
    "There are two basic routes I could take. I could perform node level prediction by treating each transcript individually and seeing which is the closest match during prediction. The other option would be to perform graph level prediction by storing all of the similarly labeled transcripts together and then using the shape of the graph for comparison. \n",
    "\n",
    "- Nodes (Items, People, Locations, Cars, ETC)\n",
    "- Edges (Connections, Interactions, Similarity, ETC)\n",
    "    - Levenshtein distance over titles?\n",
    "    - Number of similar named entities from SpaCy NER?\n",
    "- Node Features (Attributes)\n",
    "- Labels (Node-Level, Edge-Level, Graph-Level, etc)\n",
    "    - I am going to first try node-level prediction as it makes more sense to me. For this I am going to use the 'course' feature in the above pandas dataframe. This will be the target that I try to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    # It takes 21 min to run SpaCy preprocessing over each record in the data\n",
    "    khan['transcript_cleaned'] = khan['transcript'].progress_apply(lambda x: preprocessor.clean(x))\n",
    "    print(khan['transcript_cleaned'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s: str) -> list:\n",
    "    s = s.replace('[', '')\n",
    "    s = s.replace(']', '')\n",
    "    s = s.replace('\\'', '')\n",
    "    s = s.split(', ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "    # clear up the lists to be unique only.\n",
    "    khan['transcript_cleaned'] = khan['transcript'].progress_apply(lambda x: list(set(str_to_list(x))))\n",
    "    print(len(khan['transcript_cleaned'][0]))\n",
    "    print(khan['transcript_cleaned'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "# if not False:\n",
    "    khan['transcript_n_entries'] = khan['transcript_cleaned'].progress_apply(lambda x: len(x))\n",
    "    khan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# khan = khan.drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not joined_exists:\n",
    "# if not False:\n",
    "    khan.to_csv(\"khan_joined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook\n",
    "\n",
    "\"\"\"\n",
    "    nodes closer to yourself are more important. Take the incoming embeddings and dotproduct them with your own embedding.\n",
    "    Softmax across all the dot products of the nodes coming in to have it be a probability.\n",
    "    \n",
    "    Need to build the corpus of all input words.\n",
    "    \n",
    "    Use spacy to get corpus for each doc with stopwords stripped and bigrams / trigrams joined. \n",
    "    \n",
    "    Could use skipgram or bag of words with word2vec to embed words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khan['course'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# khan['transcript_cleaned'] = khan['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = khan['transcript_cleaned'].sum(axis=0)\n",
    "if isinstance(corpus, str):\n",
    "    # means it performed string concatenation so it needs to be cleaned up\n",
    "    print(corpus[0:100])\n",
    "    corpus = str_to_list(corpus)\n",
    "    print(corpus[0: 25])\n",
    "    print(len(corpus))\n",
    "    print(type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the corpus to a set since there should be no unique values\n",
    "corpus = set(corpus)\n",
    "print(len(corpus))\n",
    "corpus = list(corpus)\n",
    "print(corpus[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(khan['course'].value_counts())\n",
    "print(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LDA Normal\n",
    "temp = [d.split() for d in corpus]\n",
    "print(type(temp))\n",
    "words = corpora.Dictionary(temp)\n",
    "words.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "corpus = [words.doc2bow(doc) for doc in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=15,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, words, mds='mmds', R=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_corpus = words.doc2bow(validation['transcript'][0].split())\n",
    "print(len(new_text_corpus))\n",
    "prediction = lda_model.get_document_topics(new_text_corpus)\n",
    "prediction.sort(key = lambda x: x[1], reverse = True)\n",
    "print(prediction)\n",
    "prediction = prediction[0][0]\n",
    "print(\"Predicted Topic: %d\" % prediction)\n",
    "lda_model.show_topic(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
